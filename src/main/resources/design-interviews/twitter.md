Below is a **fully-self-contained, GitHub-ready Markdown study guide** for the classic *â€œDesign Twitterâ€* question.
I updated every â€œsee Xâ€ cross-reference so you no longer need to jump to other docs, and I added a quick-reference *
*cheat-sheet table** of conversions and rule-of-thumb numbers you should memorize.

---

# Designing â€œTwitterâ€ - System Design Study Guide

> Difficulty  **Medium**   â€¢ Target interview: Coinbase, FAANG, FinTech
> Goal Explain how to post, store, serve, and scale 100 M tweets/day with 28 B timeline reads/day.

---

## 1. What *is* Twitter?

Twitter (now *X*) is a micro-blogging network where registered users publish short messages (â‰¤280 chars) called **tweets
**. Visitors can:

* **Post** tweets (optionally with photos / videos).
* **Follow** other accounts to receive their tweets in a personal **timeline**.
* **Like** (favorite) tweets.
* Consume via Web, mobile apps, SMS, public APIs.

---

## 2. Requirements & Goals

| Type                        | Requirement                                                                                                                                            |
|-----------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Functional**              | *Post* tweet, *follow/unfollow* users, *like* tweets, build **timeline** (top N recent tweets from followees), support embedded media (photo / video). |
| **Non-functional**          | 99.95 % availability â€¢ p95 timeline latency â‰¤ 200 ms â€¢ write latency â‰¤ 50 ms. Consistency may be slightly relaxed (eventual OK).                       |
| **Extended - nice to have** | Search tweets â€¢ Replies â€¢ Retweets â€¢ Trends â€¢ â€œWho to followâ€ suggestions â€¢ Notifications â€¢ Moments.                                                   |

---

## 3. Scale & Capacity Estimates

**Users & traffic**

| Metric                   | Assumption                                                                                             | Result |
|--------------------------|--------------------------------------------------------------------------------------------------------|--------|
| Total users              | 1 B                                                                                                    |        |
| Daily active users (DAU) | 200 M                                                                                                  |        |
| New tweets/day           | 100 M â‡’ **â‰ˆ1.2 k writes/s** (Ã—3 â‡’ 3 kâ€“4 k peak)                                                        |        |
| Likes/day                | 5 likes/DAU â‡’ 1 B                                                                                      |        |
| Timeline views/day       | Each DAU reads timeline twice + 5 profile visits Ã— 20 tweets   â†’ 28 B â‡’ **â‰ˆ325 k reads/s** (â‰ˆ1 M peak) |        |

**Storage**

* Tweets: 140 chars â‰ˆ 280 B + 30 B meta â‡’ 310 B.
  100 M Ã— 310 B â‰ˆ **30 GB/day** text (no media).
* Media: 1 photo/5 tweets @ 200 kB, 1 video/10 tweets @ 2 MB â‡’ â‰ˆ **24 TB/day**.

5 years of tweets: 30 GB Ã— 365 Ã— 5 â‰ˆ 55 TB (text-only). Media â†’ multi-petabyte (handled by S3/Lake).

**Bandwidth**

*Ingress*: 24 TB/day â‰ˆ 290 MB/s
*Egress*: 35 GB/s peak (CDN for media, edge cache for hot tweets).

---

## 4. Public API Example (REST)

```http
POST /v1/tweets
Headers: Authorization: Bearer <JWT>

{
  "text": "gm world ğŸŒ",
  "media_ids": ["abc123", "def456"],
  "reply_to": "987654321",
  "location": {"lat": 37.77, "lon": -122.4}
}
```

*Response*: `201 Created` â†’ `{ "tweet_url": "https://x.com/123456" }`

---

## 5. High-Level Architecture

```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Load Balancer â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 HTTPS
                (REST)
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Tweet API Tier â”‚  â† horizontal Go/Java servers
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Kafka produce â”‚ (tweets_raw)
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Kafka Cluster â”‚  â† write 1.2k/s, read fan-out
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        Flink / Spark Enrichment
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Timeline Fan- â–‘â–‘ streaming job
          â”‚   out Service   â”‚  (push to followers, update cache)
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           Redis / Memcached (recent timelines, hot tweets)
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Object store   â”‚ media (S3+CloudFront)
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          Iceberg tables in S3 for long-term analytics
```

---

## 6. Database Schema (choice & rationale)

| Table                                                                | Reasoning / Storage                                                                   |
|----------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| **Users** (`user_id PK`, profile fields)                             | Rare writes, frequent reads â‡’ RDBMS (Aurora / Spanner) or strongly-consistent Doc DB. |
| **Tweets** (`tweet_id PK`, `user_id`, `created_at`, `text`, `media`) | Massive append-only â‡’ wide-column store (Cassandra) **sharded by `tweet_id`**.        |
| **Follows** (`user_id`, `followee_id`)                               | Small rows, need existence checks â‡’ DynamoDB partitioned by `user_id`.                |
| **Likes** (`user_id`, `tweet_id`)                                    | High write, idempotent â‡’ DynamoDB with TTL for unlikes.                               |

*SQL vs NoSQL*: Tweets demand horizontal scale + high write; follows & likes need low-latency lookups. Use polyglot:
Cassandra / ScyllaDB for tweets, DynamoDB for edges.

---

## 7. Sharding Strategy

**TweetID = (Time-ordered ULID64)**
`[48-bit epoch seconds][16-bit sequence]`

1. **Primary key** naturally sorts by time â†’ efficient â€œlatest tweetsâ€ scan.
2. **Shard** by `(tweet_id % N_shards)`; new tweets distribute evenly.
3. Hot celebrity problem mitigated by sequence bits.

**Alternative**: shard by `user_id`; simpler reads per user but hot-key risk.

---

## 8. Caching

| Layer    | What is cached                                     | TTL             | Replacement |
|----------|----------------------------------------------------|-----------------|-------------|
| L1 Redis | â€œtimeline\:user\_idâ€ â†’ list\<tweet\_id> (latest N) | 30 s            | LRU         |
| L2 Redis | individual tweet objects                           | 10 min          | LRU         |
| CDN      | media (images/video)                               | 1 h (immutable) | N/A         |

Cache miss â†’ query Cassandra (tweets) + Dynamo (follows) â†’ recalc timeline.

---

## 9. Timeline Generation (self-contained)

### Push vs Pull

| Strategy | How                                                                                        | Pros                        | Cons                                     |
|----------|--------------------------------------------------------------------------------------------|-----------------------------|------------------------------------------|
| **Push** | On write, insert tweet-id into *each followerâ€™s* timeline list (Redis list or Flink state) | O(1) read (just a list pop) | Write amplification (avg 200 fan-outs)   |
| **Pull** | On read, fetch K followees, merge their last M tweets by recency                           | Cheap writes                | Read heavy; scatter-gather across shards |

**Hybrid**: Push for heavy users (celeb followees), pull for long-tail.

---

## 10. Fault Tolerance & Replication

* **Kafka**: `replication.factor=3`, `min.insync.replicas=2`.
* **Cassandra**: RF = 3 across AZs, QUORUM reads (timeline OK with slight staleness).
* **Redis**: Cluster mode, multi-AZ, async replicas.
* **Media**: S3 versioning + cross-region replication.

---

## 11. Load Balancing

* Client â†’ Edge: ALB + WAF â†’ API pods (round-robin, health-probe).
* API â†’ Kafka: produce w/ batching (`linger.ms=5`, `batch.size=64 kB`).
* Timeline WS: sticky hash on user\_id â†’ gateway pod; Envoy handles WebSocket upgrades.

---

## 12. Monitoring & Alerts

| Metric                      | Alert Threshold |
|-----------------------------|-----------------|
| p95 tweet POST latency      | > 100 ms        |
| p95 timeline latency        | > 200 ms        |
| Kafka consumer lag          | > 10 s          |
| Redis hit ratio             | < 80 %          |
| Cassandra read timeout rate | > 1 %           |

Log lineage for GDPR deletions & internal audits.

---

## 13. Extensions (self-filled)

### Retweet

Store `retweet_id` + pointer to original `tweet_id`. Timeline service expands inline; no text duplication.

### Trending Hashtags

*Windowed Flink* job over Kafka counting hashtags per 5-min sliding window; top N stored in Redis.

### Search

Create **Lucene / OpenSearch** cluster; job streams tweets â†’ inverted index for full-text + hashtag filters.

### Who-to-Follow

Offline Spark ML job using follow-graph embeddings; features stored in Redis; API returns top suggestions.

---

## 14. Memorize-Me Cheat-Sheet

| Quantity         | Value                              |
|------------------|------------------------------------|
| 1 KB             | 1024 bytes                         |
| 1 MB             | 1024 KB â‰ˆ 1 M bytes                |
| 1 GB             | 1024 MB â‰ˆ 1 B bytes                |
| 1 TB             | 1024 GB â‰ˆ 1 T bytes                |
| Secs in day      | 86 400                             |
| Tweets/day 100 M | â‰ˆ1.2 k tweets/s (avg)              |
| 28 B reads/day   | â‰ˆ325 k reads/s (avg)               |
| Like ratio       | 5 likes/DAU â†’ 1 B/day              |
| Media â‰ˆ24 TB/day | 278 MB/s ingress                   |
| p95 lat targets  | 50 ms write Â· 200 ms read          |
| Kafka safe RF    | 3 (2 in-sync)                      |
| Redis RAM        | 1 byte key overhead â‰ˆ 16â€“48 B item |

---

**Practice**: Re-draw the high-level boxes in 60 s, then verbally walk through write-path, read-path, failure scenario,
and one extended feature. Thatâ€™s the polished answer interviewers expect.
